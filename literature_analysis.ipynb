{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 70 # seed for NMF topic model\n",
    "num_topics = 12\n",
    "query = 'title-abs-key(\"predictive maintenance\")'\n",
    "labels = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n",
    "vis_seed = 6 # seed for t-SNE visualization\n",
    "vis_angle = 135 # rotation angle for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gensim\n",
    "import scopus\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition\n",
    "import sklearn.feature_extraction\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from IPython.display import display\n",
    "from collections import defaultdict\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set('paper')\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# some python 3 trickery\n",
    "import sys\n",
    "if sys.version_info[0] >= 3:\n",
    "    unicode = str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fig(w=1, h=None):\n",
    "    if h is None: h = w\n",
    "    figsize = (6 * w, 3 * h)\n",
    "    sns.set(rc={'figure.figsize': figsize})\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    plt.clf()\n",
    "    return fig\n",
    "\n",
    "def top_k(mapping, k=10):\n",
    "    return sorted(mapping.keys(), key=lambda x: mapping[x])[::-1][:k]\n",
    "\n",
    "pd.set_option('display.max_rows', 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eids = scopus.ScopusSearch(query).get_eids()\n",
    "random.seed(0)\n",
    "random.shuffle(eids)\n",
    "\n",
    "bar = widgets.IntProgress(\n",
    "    min=0, \n",
    "    max=len(eids), \n",
    "    description='Loading')\n",
    "display(bar)\n",
    "\n",
    "print('query: {} ({} results)'.format(query, len(eids)))\n",
    "\n",
    "papers = []\n",
    "for eid in eids:\n",
    "    papers.append(scopus.ScopusAbstract(eid, view='FULL'))\n",
    "    bar.value += 1\n",
    "    bar.description = str(bar.value)\n",
    "\n",
    "print('scopus returned {} results'.format(len(papers)))\n",
    "\n",
    "# Filter on article type\n",
    "# ar = article, cp = conference proceeding, re = review\n",
    "print(set([p.citationType for p in papers]))\n",
    "\n",
    "# Filter on citation type\n",
    "#papers = [p for p in papers if p.citationType in ('ar', 'cp', 're') or p.citationType is None]\n",
    "\n",
    "# Filter on date\n",
    "#papers = [p for p in papers if int(p.coverDate[:4]) <= 2018]\n",
    "\n",
    "print('{} papers remaining'.format(len(papers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for p in papers:\n",
    "    text = (p.title or '') + ' ' + (p.abstract or '')\n",
    "    text = text.lower()\n",
    "    text = re.sub('[^a-zA-Z0-9]', ' ', text) # Replace punctation by spaces\n",
    "    texts.append([w for w in text.split(' ') if w]) # Split on spaces, remove empty items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per year\n",
    "year_count = defaultdict(int)\n",
    "\n",
    "for p in papers:\n",
    "    year_count[int(p.coverDate[:4])] += 1\n",
    "       \n",
    "years = range(2000, 2020)\n",
    "\n",
    "prepare_fig(2.5, 1)\n",
    "plt.xlabel(\"No. publications\")\n",
    "plt.bar(\n",
    "    years,\n",
    "    [year_count[y] for y in years])\n",
    "plt.xticks(years);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per aggregation type\n",
    "def plot_statistic(fun):\n",
    "    count = defaultdict(int)\n",
    "\n",
    "    for p in papers:\n",
    "        for key in fun(p):\n",
    "            if key:\n",
    "                count[unicode(key)] += 1\n",
    "\n",
    "    top_keys = top_k(count, 50)\n",
    "\n",
    "    prepare_fig(1, 4)\n",
    "    plt.xlabel(\"No. publications\")\n",
    "    plt.barh(\n",
    "        range(len(top_keys)),\n",
    "        [count[a] for a in top_keys])\n",
    "    plt.yticks(\n",
    "        range(len(top_keys)), \n",
    "        [key[:50] for key in top_keys])\n",
    "    plt.show()\n",
    "\n",
    "plot_statistic(lambda p: [p.aggregationType])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per author\n",
    "plot_statistic(lambda p: set(a.indexed_name for a in p.authors or []))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per institute\n",
    "def clean_affiliation(name):\n",
    "    name = unicode(name).title()\n",
    "    pairs = [\n",
    "        ['University', 'U'],\n",
    "        ['Universitat', 'U'],\n",
    "        ['Laboratories', 'Lab'],\n",
    "        ['Laboratory', 'Lab'],\n",
    "        ['National', 'Nat'],\n",
    "        ['Corporation', 'Corp'],\n",
    "        ['Technology', 'Tech'],\n",
    "        ['Institute', 'Inst'],\n",
    "    ]\n",
    "    \n",
    "    for needle, replacement in pairs:\n",
    "        name = name.replace(needle, replacement)\n",
    "    return name\n",
    "\n",
    "plot_statistic(lambda p: set(clean_affiliation(a.affilname) for a in p.affiliations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications per publication source, conference/journal (TODO: cleaning!)\n",
    "plot_statistic(lambda p: [p.publicationName])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load stopwords, bigrams, and stem rules\n",
    "try:\n",
    "    stopwords = set()\n",
    "    with open('stopwords.txt', 'r') as f:\n",
    "        stopwords = [w.strip() for w in f if w.strip()]\n",
    "                \n",
    "    print('loaded {} stopwords'.format(len(stopwords)))\n",
    "except Exception as e:\n",
    "    logging.error('failed to load stopwords.txt: {}'.format(e))\n",
    "\n",
    "try:\n",
    "    bigrams = dict()\n",
    "    with open('bigrams.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                a, b, c = line.split()\n",
    "                bigrams[a, b] = c\n",
    "        \n",
    "    print('loaded {} bigrams'.format(len(bigrams)))\n",
    "except Exception as e:\n",
    "    logging.error('failed to load bigrams.txt: {}'.format(e))\n",
    "    \n",
    "try:\n",
    "    stem_rules = dict()\n",
    "    with open('stemming.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                a, b = line.strip().split()\n",
    "                stem_rules[a] = b\n",
    "        \n",
    "    print('loaded {} stem rules'.format(len(stem_rules)))\n",
    "except Exception as e:\n",
    "    logging.error('failed to load stemming.txt: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print common words\n",
    "one_count = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for a in text:\n",
    "        one_count[a] += 1\n",
    "        \n",
    "print('Top words')\n",
    "display(pd.DataFrame(\n",
    "    [(w, one_count[w], 'Yes' * (w in stopwords)) for w in top_k(one_count, 250)],\n",
    "    columns=['word', 'count', 'in stopwords.txt?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print common bigrams\n",
    "two_count = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for a, b in zip(text, text[1:]):\n",
    "        if a not in stopwords and b not in stopwords:\n",
    "            two_count[a, b] += 1\n",
    "            \n",
    "print('Top bigrams')\n",
    "display(pd.DataFrame(\n",
    "    [(w, two_count[w], 'Yes' * (w in bigrams)) for w in top_k(two_count, 250)],\n",
    "    columns=['bigram', 'count', 'in bigrams.txt?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge bigrams\n",
    "for text in texts:\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(text) - 1:\n",
    "        a, b = text[i], text[i + 1]\n",
    "        \n",
    "        if (a, b) in bigrams:\n",
    "            text[i:i+2] = [bigrams[a,b]]\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter tokens\n",
    "def filter_texts(texts, f):\n",
    "    return [[token for token in text if f(token)] for text in texts] \n",
    "\n",
    "texts = filter_texts(texts, lambda t: t not in stopwords) # Filter stopwords\n",
    "texts = filter_texts(texts, lambda t: len(t) > 1) # Remove single char words \n",
    "texts = filter_texts(texts, lambda t: not re.match('^[0-9]+$', t)) # Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "stemmer = gensim.parsing.PorterStemmer()\n",
    "word_count = defaultdict(int)\n",
    "stemming = dict()\n",
    "unstemming = dict()\n",
    "\n",
    "\n",
    "# Stem each word and count \"word, stem\" pairs\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        word_count[token] += 1\n",
    "    \n",
    "# Sorted stems by frequency, the value for unstemming[stem] is overwritten \n",
    "# and should be the one with the highest count.\n",
    "for token in sorted(word_count.keys(), key=word_count.get):\n",
    "    stem = stemmer.stem(token)\n",
    "    stemming[token] = stem\n",
    "    unstemming[stem] = token\n",
    "\n",
    "# Overwrite with user defined rules\n",
    "for a, b in stem_rules.items():\n",
    "    stemming[a] = b\n",
    "    unstemming[b] = b\n",
    "\n",
    "# Stem words\n",
    "stemmed_texts = [[unstemming[stemming[token]] for token in text] for text in texts]\n",
    "\n",
    "logging.info('stemming reduced {} to {} tokens'.format(len(stemming), len(unstemming)))\n",
    "pd.DataFrame(\n",
    "    [(\n",
    "        token, \n",
    "        stemming[token], \n",
    "        unstemming[stemming[token]], \n",
    "        word_count[token]\n",
    "    ) for token in top_k(word_count, k=250)],\n",
    "    columns=['Original', 'Stem', 'Unstem', 'Count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove papers with less than 25 words after preprocessing\n",
    "n = len(papers)\n",
    "indices = [i for i in range(len(papers)) if len(stemmed_texts[i]) > 25]\n",
    "papers = [papers[i] for i in indices]\n",
    "final_texts = [stemmed_texts[i] for i in indices]\n",
    "\n",
    "print('papers: {} (removed {})'.format(len(papers), n - len(papers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = gensim.corpora.Dictionary(final_texts)\n",
    "dic.filter_extremes(0, 0.5) # Remove \n",
    "dic.filter_extremes(5, 1)   # \n",
    "corpus = [dic.doc2bow(text) for text in final_texts]\n",
    "\n",
    "print('papers: {}'.format(len(corpus)))\n",
    "print('dictionary size: {}'.format(len(dic)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frequency matrix\n",
    "n, m = len(corpus), len(dic)\n",
    "matrix = np.zeros((n, m))\n",
    "\n",
    "for i, row in enumerate(corpus):\n",
    "    for j, freq in row:\n",
    "        matrix[i,j] = freq\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run TFIDF model\n",
    "tfidf_model = sklearn.feature_extraction.text.TfidfTransformer()\n",
    "tfidf_matrix = tfidf_model.fit_transform(matrix).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_model = sklearn.decomposition.NMF(\n",
    "    n_components=num_topics,\n",
    "    random_state=seed,\n",
    "    tol=1e-9,\n",
    "    max_iter=500,\n",
    "    verbose=True)\n",
    "\n",
    "# Train model\n",
    "doc2topic = nmf_model.fit_transform(tfidf_matrix)\n",
    "topic2token = nmf_model.components_\n",
    "\n",
    "topic_norm = np.sum(topic2token, axis=1)\n",
    "topic2token /= topic_norm[:,np.newaxis]\n",
    "doc2topic *= topic_norm[np.newaxis,:]\n",
    "\n",
    "doc_norm = np.sum(doc2topic, axis=1)\n",
    "doc2topic /= doc_norm[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for label, vec in zip(labels, topic2token):\n",
    "    rows.append([label] + ['{} ({:.2})'.format(dic[i], vec[i]) for i in np.argsort(vec)[::-1][:10]])\n",
    "\n",
    "# Each row is a topic, columns are words ordered by weight \n",
    "pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_fig(2, 3)\n",
    "for index in range(num_topics):\n",
    "    mapping = dict()\n",
    "    for i in np.argsort(topic2token[index])[::-1][:100]:\n",
    "        if topic2token[index,i] > 0:\n",
    "            mapping[dic[i]] = topic2token[index,i]\n",
    "    \n",
    "    def get_color(word, **kwargs):\n",
    "        weight = kwargs['font_size'] / 60.0 * 0.6 + 0.4\n",
    "        r, g, b = plt.get_cmap('Blues')(weight)[:3]\n",
    "        return 'rgb(%s, %s, %s)' % (int(r * 255), int(g * 255), int(b * 255))\n",
    "    \n",
    "    wc = WordCloud(\n",
    "        prefer_horizontal=True,\n",
    "        max_font_size=75,\n",
    "        #width=395,\n",
    "        #height=250,\n",
    "        scale=2,\n",
    "        background_color='white', \n",
    "        color_func=get_color, \n",
    "        relative_scaling=0.5)\n",
    "    wc.fit_words(mapping)\n",
    "    \n",
    "    print('Topic {} ({})'.format(index, labels[index]))\n",
    "    plt.subplot(4, 3, index + 1)\n",
    "    plt.imshow(wc.to_array(), interpolation='bilinear')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "plt.subplots_adjust(left=0, right=1, top=1, bottom=0, wspace=0.1, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.manifold\n",
    "import sklearn.metrics.pairwise\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "def draw_dot(p, t, zorder=0):\n",
    "    color = plt.get_cmap('jet')(float(t) / num_topics)\n",
    "    color = 0.8 * np.array(color)[:3]\n",
    "    \n",
    "    plt.scatter(\n",
    "        p[0], \n",
    "        p[1],\n",
    "        s=150,\n",
    "        c=[color],\n",
    "        marker='o',\n",
    "        linewidth=0.5,\n",
    "        zorder=zorder)\n",
    "    \n",
    "    plt.text(\n",
    "        p[0], \n",
    "        p[1],\n",
    "        labels[t],\n",
    "        fontsize=6,\n",
    "        color='1',\n",
    "        va='center',\n",
    "        ha='center',\n",
    "        fontweight='bold',\n",
    "        zorder=zorder + 1)\n",
    "\n",
    "# Lower dimensionality of original frequency matrix to improve cosine distances for visualization\n",
    "reduced_matrix = TruncatedSVD(\n",
    "    n_components=10, \n",
    "    random_state=seed\n",
    ").fit_transform(tfidf_matrix)\n",
    "\n",
    "# Learn model\n",
    "model = sklearn.manifold.TSNE(\n",
    "    verbose=True,\n",
    "    metric='cosine',\n",
    "    random_state=vis_seed,\n",
    "    perplexity=20)\n",
    "pos = model.fit_transform(reduced_matrix)\n",
    "\n",
    "# Rotate visualization\n",
    "theta = np.deg2rad(vis_angle + 60)\n",
    "R = np.array([[np.cos(theta), np.sin(theta)], \n",
    "              [-np.sin(theta), np.cos(theta)]])\n",
    "pos = np.dot(pos, R)\n",
    "\n",
    "# Resize so xy-position is between 0.05 and 0.95\n",
    "pos -= (np.amin(pos, axis=0) + np.amax(pos, axis=0)) / 2\n",
    "pos /= np.amax(np.abs(pos))\n",
    "pos = (pos * 0.5) + 0.5\n",
    "pos = (pos * 0.9) + 0.05\n",
    "\n",
    "prepare_fig(2, 4)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "zorder = 0\n",
    "\n",
    "# Draw dots\n",
    "for i in np.random.permutation(len(doc2topic)):\n",
    "    topic_id = np.argmax(doc2topic[i])\n",
    "    draw_dot(pos[i], topic_id, zorder)\n",
    "    zorder += 2\n",
    "\n",
    "# Draw legend\n",
    "for i in range(num_topics):    \n",
    "    y = 0.985 - i * 0.02\n",
    "    label = ', '.join(dic[w] for w in np.argsort(topic2token[i])[::-1][:3])\n",
    "\n",
    "    draw_dot([0.015, y], i)\n",
    "    plt.text(0.03, y, label, ha='left', va='center', fontsize=8, zorder=zorder)\n",
    "    zorder += 1\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def on_select(i):\n",
    "    p = papers[i]\n",
    "    print(p.title)\n",
    "    \n",
    "    prepare_fig(1, 2)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(pos.T[0], pos.T[1], c='.5')\n",
    "    plt.scatter(pos[i,0], pos[i,1], c='0', marker='x')\n",
    "    plt.show()\n",
    "\n",
    "    top_words = np.argsort(topic2token, axis=1)[:,::-1]\n",
    "    ticks = ['{} ({})'.format(l, ', '.join(dic[v] for v in w[:3])) for l, w in zip(labels, top_words)]\n",
    "    \n",
    "    fig = prepare_fig(1, 2)\n",
    "    plt.barh(range(num_topics), doc2topic[i])\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(num_topics, -1)\n",
    "    plt.yticks(range(num_topics), ticks)\n",
    "    plt.show()\n",
    "    \n",
    "    print(p.abstract)\n",
    "\n",
    "options = sorted([((p.title or '').strip()[:50], i) for i, p in enumerate(papers)])\n",
    "widget = widgets.interactive(on_select, i=options)\n",
    "widget.children[-1].layout.height = '2000px'\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
